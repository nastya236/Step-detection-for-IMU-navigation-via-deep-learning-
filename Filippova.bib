% Encoding: UTF-8

@Article{Hang2019,
  author      = {Hang Yan and Sachini Herath and Yasutaka Furukawa},
  title       = {RoNIN: Robust Neural Inertial Navigation in the Wild:
Benchmark, Evaluations, and New Methods},
  abstract    = {This paper sets a new foundation for data-driven inertial navigation research, where the task is the estimation of
positions and orientations of a moving subject from a sequence of IMU sensor measurements. More concretely, the
paper presents 1) a new benchmark containing more than
40 hours of IMU sensor data from 100 human subjects with
ground-truth 3D trajectories under natural human motions;
2) novel neural inertial navigation architectures, making
significant improvements for challenging motion cases; and
3) qualitative and quantitative evaluations of the competing
methods over three inertial navigation benchmarks. We will
share the code and data to promote further research.},
  date        = {2019-05-30},
  eprint      = {https://arxiv.org/pdf/1905.12853v1},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1905.12853v1:PDF},
  keywords    = {cs.CV, stat.ML},
}

@Article{Theis2015,
  author      = {Lucas Theis and Matthias Bethge},
  title       = {Generative Image Modeling Using Spatial LSTMs},
  abstract    = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
  date        = {2015-06-10},
  eprint      = {http://arxiv.org/abs/1506.03478v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1506.03478v2:PDF},
  keywords    = {stat.ML, cs.CV, cs.LG},
}

@InProceedings{Tresp01mixturesof,
  author    = {Volker Tresp},
  title     = {Mixtures of Gaussian processes},
  booktitle = {Advances in Neural Information Processing Systems 13},
  year      = {2001},
  pages     = {654--660},
  publisher = {MIT Press},
}

@Article{Shazeer2017,
  author      = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  title       = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
  abstract    = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  date        = {2017-01-23},
  eprint      = {http://arxiv.org/abs/1701.06538v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1701.06538v1:PDF},
  keywords    = {cs.LG, cs.CL, cs.NE, stat.ML},
}

@InCollection{NIPS2009_3641,
  author    = {Bangpeng Yao and Dirk Walther and Diane Beck and Li Fei-fei},
  title     = {Hierarchical Mixture of Classification Experts Uncovers Interactions between Brain Regions},
  booktitle = {Advances in Neural Information Processing Systems 22},
  publisher = {Curran Associates, Inc.},
  year      = {2009},
  editor    = {Y. Bengio and D. Schuurmans and J. D. Lafferty and C. K. I. Williams and A. Culotta},
  pages     = {2178--2186},
  url       = {http://papers.nips.cc/paper/3641-hierarchical-mixture-of-classification-experts-uncovers-interactions-between-brain-regions.pdf},
}

@Article{RuDaCoP,
  author      = {Rahaf Aljundi and Punarjay Chakravarty and Tinne Tuytelaars},
  title       = {RuDaCoP: The Dataset for Smartphone-based
Intellectual Pedestrian Navigation},
  abstract    = {This paper presents the large and diverse dataset
for development of smartphone-based pedestrian navigation
algorithms. This dataset consists of about 1200 sets of inertial
measurements from sensors of several smartphones. The
measurements are collected while walking through different
trajectories up to 10 minutes long. The data are accompanied by
the high accuracy ground truth collected with two foot-mounted
inertial measurement units and post-processed by the presented
algorithms. The dataset suits both for training of intellectual
pedestrian navigation algorithms based on learning techniques
and for development of pedestrian navigation algorithms based
on classical approaches. The dataset is accessible at
http://gartseev.ru/projects/ipin2019.},
  date        = {2019-10-03},
  eprint      = {https://arxiv.org/pdf/1908.03609},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  file        = {},
  keywords    = {},
}
